\documentclass[11pt,english,a4paper]{article}
\usepackage{babel}
\input{/home/marius/Dokumenter/preamples/phys_en.pre}
\usepackage{lstautogobble}
\usepackage{csquotes}
\author{\normalsize Marius Jonsson \\\\
\vspace{5px}
\normalsize \texttt{http://github.com/kingoslo/postmann.pat}}
\title{\bf \uppercase{Study of the implementation of an elementary variational monte-carlo scheme and developments of the blocking method.}}
\date{\normalsize \today}
\addbibresource{/home/marius/Dokumenter/MyLibrary.bib}
\DeclareUnicodeCharacter{2212}{$-$}
\begin{document}
\maketitle
\begin{abstract} \normalsize This is a report submission for the second project of «Computational physics 2» at the Institute of Physics, University of Oslo, autumn 2016.\\
\\
We implement a variational Monte-Carlo scheme using Hastings-Metropolis algorithm to compute observables for a quantum dot of $n=2,6,12,20$ interacting electrons at frequency $\omega = 1,0.5,0.1,0.05,0.01$. Un particular, we found the trail energy of a Slater-Jastrow type trail state function for theory of a harmonic oscillator potential with Couloumb repulsion. We also found mean distance between electrons, their expected kinetic- and potential energy. We found ideal variational parameters for the trail state function. We computed that the proportion of the load which is parallelizable for our scheme is $\E{f} = 0.9999988$ with variance $\Var{f} = 6 \cdot 10^{-14}$. We fond that expected maximal speed up was 800 000 for $10^{6}$ Monte-Carlo cycles. We found that there was no effect of vectorization ($p=0.7634$ df = 18). Furthermore, we develop the mathematics of the Blocking method. We prove that the error of the blocking method is reduced with every iteration and it eventually becomes constant once the observations become independent. We conjecture that it should be possible to strengthen the result further. We obtain a formula which determines the rate of convergence. We conjecture that it is possible to obtain a formula determining the ideal number of blocking iterations.
\end{abstract}
\section*{\uppercase{Introduction}}
The implementation of a Variational Monte Carlo scheme using Hastings-Metropolis algorithm is possibly the most robust way of estimating observables if a proposal of the state function $\psi_T$ exist. It's robustness, as we shall see, is mainly due to that the properties and construction of stationary Markov chains are well understood. Their construction was first made made popular by a paper \cite{nicholas_metropolis_equation_1953}. In 1971, Canadian statistician Wilfred Keith Hastings provided modern rigorous proof of what became the Hastings-Metropolis theorem, contained in the appendix. His proof was constructive, in the sense that it shows that any irreducible Markov chain generates a time reversible Markov chain, $\{X_i, i \in \mathbb{N}\}$, with stationary probability distribution $\psi_T$. Using $X_i$ we can compute any desired statistics, such as the distribution of the spectrum of the energy $E_i$. The application to quantum theory is a corollary of Chebyshev's inequality. It proves that $\E{E} = \lim_{m \to \infty} (1/m) \sum_{i=1}^m E_i$ with convergence in probability \parencite{varadhan_probability_2001}. That means the observations $E_i$ immediately represent estimates for any observable! In this project we will investigate the ground state of a quantum dot with $n\geq 2$ electrons. Assuming that $r_{ij} = |\vec{r}_i - \vec r_j|$ is the distance between electron $i$ and $j$, our theory is 
\begin{equation}
E_T = \sum_{i=1}^N \left[ -\frac{1}{2} \frac{\nabla^2_i \psi_T}{\psi_T} + \frac{1}{2} \omega^2 r_i^2 + \sum_{j=i+1}^N \frac{1}{r_{ij}} \right], \label{eq:ET}
\end{equation}
\begin{equation}
\psi_T = C\psi_0 \psi_C, \quad  \psi_0(\vec{r}_1, \vec{r}_2) = \exp \left(-\frac{1}{2} \alpha  \omega  \left( {{r}_1}^2+  {{r}_2}^2\right)\right),  \quad  \psi_C(\vec{r}_1, \vec{r}_2) =  \exp \left( \frac{r_{  12}}{\beta  r_{  12}+1} \right). \label{eq:psiT}
\end{equation}
We will generalize $\psi_T$ for any natural number of electrons. We assume the parameters $\alpha,\beta$ are given. In our case they are not known, but will be fixed such that $\E{E(\alpha,\beta)}$ is a minimum with respect to $\alpha,\beta$.\\
\\
Although the variational Monte-Carlo scheme is invaluable to any physicist interested in computational physics, the highlights of this report are new contributions to the theory of the Blocking method. In the evaluation of the implementation of the variational Monte-Carlo scheme, we will use the Blocking method to compute the variance of the means of our statistics, for example $\Var{\E{E}}$. In the results we develop the mathematics to contain 3 immediate lemmas of the work by Flyvbjerg and Petersen, and two new propositions. We aimed to provide proof of the Blocking method. At the time of writing its not clear whether such a proof exist already. Unfortunately I was pressed on time, and finalizing the proof of was a bit more difficult than first anticipated. However, I hope to jolt readers of this report with the strength of the new advertised propositions and their corollaries. Hopefully the proof of the Blocking method come over the summer if there is interest in developing the Blocking method.\\
\\
The report is structured by «introduction»-, «methods»-, «results and discussion»- and finally a «conclusion and perspectives»-sections.
\section*{\uppercase{Methods}}
Let's first discuss the Hastings-Metropolis theorem. Its constructive proof is contained in the appendix. Often whilst proving results about Markov chains we are interested in whether the Markov chain can reach every state from any other state. For the Hastings-Metropolis method, this is important because it ensures that all the divisions which are made in the proof are non-zero. Let's make this notion precise. Suppose $q_{  ij}$ is the transition probability matrix of a Markov chain $X_n$ and $S$ is the state space. We say that $X_n$ is \textit{\textbf{irreducible}} if for all $i,j \in S$ there exist an $n \in \mathbb{N}$ such that $(q^n)_{  ij}, (q^n)_{  ji} > 0$.\\
\\
Suppose that $X_n$ is irreducible and not deterministically periodic, then we say that the probabilities $\pi_i = P(X_n = i)$ is the \textit{\textbf{stationary distribution}} of $X_n$.\\
\\
We will say that $X_n$ is \textit{\textbf{time reversible}} if the conditional probability $P(X_{n} = j$, given that $X_{n+1} = i) = P_{  ij}$ for all $i,j \in S$ and $n \in \mathbb{N}$. Interestingly, it is easily shown that a Markov chain is time reversible if and only if $\pi_i P_{  ij} = \pi_j P_{  ji}$ for all $i,j \in S$. If you wish to prove this yourself, the proposition follows from Markov chain property and Bayes' theorem. With these definitions, we can understand the Hastings-Metropolis theorem.
\begin{theorem}[Hastings-Metropolis theorem]
Suppose that $C\pi_i$ is a discrete probability distribution. If $q_{ij}$ is any irreducible transition probability matrix, and $X_n$ is a Markov chain with transition probability matrix 
\begin{equation}
P_{ij} = \ccases {
\alpha_{  ij} q_{  ij} , \quad &j\neq i\\
q_{  ii} + \sum_{k=0}^\infty q_{  ik}(1 - \alpha_{  ik})\quad & j=i
},\qquad \text{where} \qquad \alpha_{  ij} = \min \left( \frac{\pi_j q_{  ji}}{\pi_i q_{  ij}} , 1\right), \label{eq:metropolis}
\end{equation}
then $X_n$ is time reversible with stationary distribution $\pi_i$. \label{thm:metropolis}
\end{theorem}
It is immediate that the entire variational Monte-Carlo scheme is specified if we define $\pi_i$ and $q_{ij}$. Suppose $\vec{r}_k^{(i)}$ denote the coordinates of electron $k$ at iteration number $i$. We will take $\pi_i$ to equal $\psi_T(\vec{r}_1^{(i)},\vec{r}_2^{(i)},\cdots ,\vec{r}_n^{(i)})$ for all $i \in \mathbb{N}$. We will refer to the transition probability matrix $q_{ij}$ as the \defn{importance sampling}.\\
\\
As we noted, theorem \ref{thm:metropolis} implies that the variational Monte-Carlo scheme is uniquely defined once we specify $\psi_T$ and the importance sampling. We will take the trail state function to be a Slater-Jastrow product. This is desirable since it exhibits the correct cusp behaviour for electrons with parallel and anti-parallel spin and it means that the variational parameters has simple physical interpretation. We will see that this produces ground state energy error smaller than 0.02\%. Specifically, we define
\begin{align}
\psi_T &= \det(D_\uparrow)\det(D_\downarrow)\psi_C, \qquad \psi_C(\vec{r}_1, \vec{r}_2, \cdots, \vec{r}_n; \beta) = \exp\left( \sum_{i=1}^n\sum_{j=i+1}^n \frac{\gamma_{ij} r_{ij}}{1 + \beta  r_{ij}} \right) \label{eq:wf} \\
\text{such that}& \qquad  (D_\uparrow)_{  ij} = \phi_{2j}(\vec{r}_{2i}; \alpha), \quad (D_\downarrow)_{  ij} = \phi_{2j+1}(\vec{r}_{2i + 1};\alpha) \quad i,j \in \{0,1,\cdots,\frac{n}{2}-1\} \nonumber \\
\text{and}& \qquad \phi_{j}(\vec{r}_i;\alpha) = H_{n_{xj}}\big( (\alpha \omega)^{1/2} x_i\big) H_{n_{yj}}\big( (\alpha \omega)^{1/2} y_i\big) \exp \left( - \frac{1}{2}\alpha \omega r_i^2 \right)
\end{align}
\begin{center}
\begin{tabular}{r |c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c}
$j$ & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & $\cdots$ \\
\hline
$n_{xj}$& 0 & 0 & 1 & 1 & 0 & 0 & 2 & 2 & 1 & 1 & 0 & 0 & $\cdots$  \\
$n_{yj}$& 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 2 & 2 & $\cdots$ \\
Spin & $\uparrow$ & $\downarrow$& $\uparrow$ & $\downarrow$& $\uparrow$ & $\downarrow$& $\uparrow$ & $\downarrow$& $\uparrow$ & $\downarrow$& $\uparrow$ & $\downarrow$& $\cdots$ 
\end{tabular} \qquad for all $i \in \{0,1,2,\cdots n-1\}$.
\end{center}
It remains to define the importance sampling. The generating Markov chain with probability density $q_{ij}$ will be taken to be a stationary isotropic diffusion process. We have seen in lectures that it is immediate from the Fokker-Planck equation that if $r^{(i)}_k,r^{(j)}_k$ represent two different position of particle number $k$ and $\Delta t > 0$ is a parameter, then such a process must satisfy
\[
q_{ij} = G(\vec{r}_k^{(i)},\vec{r}_k^{(j)}; \Delta t) = \frac{1}{(2 \pi \Delta t )^{1/2}} \exp \left( - \frac{(\vec{r}_k^{(j)} - \vec{r}_k^{(j)} - \Delta t (\nabla \psi_T/\psi_T) )^2}{2\Delta t} \right), \quad \text{for all $k \in \{1,2,\cdots,n\}$}
\]
Which is nothing but the multinomial normal probability distribution with mean mean $\Delta t \nabla\psi_T / \psi_T$ and variance $\Delta t$ \parencite{devore_modern_2012}. That means if $\xi \sim \mathrm{N}(0,1)$ is some standard normal random variable we can write
\[
\vec{r}_k^{(j)} = \vec{r}_k^{(i)} + \frac{1}{2} \frac{\nabla \psi_T}{\psi_T} + \xi(\Delta t)^{1/2}. \qquad \text{\parencite[338]{degroot_probability_2012} }
\]
These expressions suffice to fix the implementation of the importance sampling, as we shall see in a practical example at the end of this section. As announced we want to estimate the value of the variational parameters $\alpha$ and $\beta$. The simplest implementation is possibly the steepest decent method.\\
\\
Suppose $f:\mathbb{R}^n \to \mathbb{R}$ is a function, then \cite[515]{press_numerical_2007} characterises the steepest decent method as a \textit{not-very-good method} that works by «Start at a point $\vec r^{(0)}$. As many times as needed, move from $\vec r^{(i)}$ to $\vec{r}^{(i+1)}$ minimizing along the line from $\vec r^{(i)}$ in the direction of the local downhill gradient $-\nabla f(\vec{r}^{(i)})$.». This means that given $\vec{r}^{(0)}$ we can iterate by
\begin{equation}
\vec{r}^{(i+1)} = \vec{r}^{(i)} - \zeta_i \nabla f(\vec{r}^{(i)}) \qquad \text{for all $i \in \mathbb{N}$ \qquad \parencite{hazewinkel_gradient_1989}} \label{eq:decent}
\end{equation}
Some choices of sequences $\{\zeta_i\}_{i=1}^\infty$ does not ensure convergence. Therefore the choice of $\zeta_i$ is interesting. Ideally we choose $\zeta_i$ such that $f(\vec{r}^{(i)} - \zeta_i \nabla f(\vec{r}^{(i)}))$ is maximised for all $i \in \mathbb{N}$ \parencite{hazewinkel_gradient_1989}. This is most conveniently done by differentiating equations \eqref{eq:steepest1} and \eqref{eq:steepest2} with respect to $\zeta_i$. By the defining equation \eqref{eq:decent} we therefore need to calculate $\nabla \E{E}$. By using the definition of expected value and pulling the derivative into the integral, we discover that for any variational parameter $\theta$,
\[
\pp \E{E};\theta; = 2 \left[ \E{\frac{E}{\psi_T} \pp \psi_T;\theta;} - \E{E} \E{\frac{1}{\psi_T} \pp \psi_T ; \theta; }  \right]
\]
Using this equation we express $\nabla \E{E}$ by the derivatives of the wave function. And estimate all the relevant expectation values by the mean. But we know how to differentiate, so this follows easily after one step of Jacobi's formula. Please see appendix for details of the derivation and the resulting equations. Using these estimates we will eventually be able to estimate the expected observables using the mean as the estimator. We will however be interested in estimating the variance of the mean to quantify the errors and produce an indication of the appropriateness of the state function. For these purposes, the blocking method is perfect.\\
\\
Suppose $\overline X_m$ is the mean of $m$ random variables and we want to compute $\Var{\overline X_m}$. The blocking method is a way of quickly computing $\Var{\overline X_m}$, and is implemented as follows: Start by taking the average of every two sequential random variables from a time-series. That means that we obtain $m/2$ new random variables. If we keep on repeating this process we will obtain $m/2^k$ random variables after $k$ repetitions. We will call each iteration the process a blocking transformation. It turns out that under certain conditions, there is some $k$, such that after $k$ blocking transformations, the variance of the blocked variables are essentially the same as $\Var{\overline X_m}$. Let's make this precise.\\
\\
Suppose that $X_1, \cdots, X_m$ are $m = 2^k$ identically distributed random variables. If we let $X^{(0)}_i = X_i$ for all $i \in \mathbb{N} = \{1,2,\cdots\}$ and
\[
X_i^{(k+1)} = \frac{1}{2} \left( X_{2i-1}^{(k)} + X_{2i}^{(k)} \right), \qquad \text{for all $k \in \mathbb{N}$}.
\]
If $k \in \mathbb{N}$ and $m^{(k)} = 2^{-k}m$, we will say that the set $\{X^{(k)}_i \;|\; i \in \{1,2,\cdots, m^{(k)}\}\}$ are \defn{subject to $\mathbold k$ blocking transformations}. We will later show that $n^{(k)} = 2^{-k}m$ is the appropriate number of random variables subject to $k$ blocking transformations. Also, we will define the autocovariance $\gamma_{i,j} = \cov (X_i,X_j)$. In the case that the Markov Chain or time series is stationary, the autocovariance is stationary. This means that $\gamma_{i,j}  = \gamma_{i+t,j+t}$ for all $i,j,t \in \mathbb{N}$ \parencite[24]{shumway_time_2011}. As a corollary of theorem \ref{thm:metropolis}, this is the case for Markov Chains of the type we are considering, in which case it is convenient to define $\gamma_t = \gamma_{i+t,j+t}$ for all $i,j,t \in \mathbb{N}$. Moreover we will let $\gamma^{(k)}_t$ denote the autocovariance subject to $k$ blocking transformations. Finally, we have everything we require to talk about the implementation of the variational Monte-Carlo scheme at program level.\\
\\
In particular, let's talk about how to implement the variational Monte-Carlo scheme in \texttt{C++}. For brevity, lets show explicitly how this is implemented for two particles, then talk about additional complexity induced by multiple particles. We have an analytical expression for $\psi_T$ from equation \eqref{eq:psiT} and expression of all the observables we want to sample. For the most elementary implementation we will only sample the energy. Details of its derivation are found in the appendix. However, the energy we arrive at using equation \eqref{eq:ET} for two particles is
\[
E_T = \frac{1}{r_{12}} + \frac{1}{2}\omega^2(1-\alpha^2)( r_1^2 + r_2^2 )
            + 2\alpha\omega + \frac{\alpha\omega r_{12}}{(1 + \beta r_{12})^2} - \frac{(1+r_{12}-\beta^2r_{12}^2)}{ r_{12}(1 + r_{12}\beta)^4 }.
\]
If we let \texttt{m}, denote the number of Monte-Carlo cycles, \texttt{r} and \texttt{rpp} denote the new and old position matrices for the two particles then a simple program using importance sampling for two particles is quickly built using the \texttt{Armadillo} library by\\
\begin{table}
\begin{lstlisting}
for (int i = 0; i < m; i++) {
    // selecting particle to move from uniform distribution
    int k = rand_particle(gen);
    int not_k = (k+1) % 2;
    
    // computing qij and qji (the importance sampling)
    rijpp = norm(rpp.col(0)-rpp.col(1));
    vec Fpp = -2*a*w*rpp.col(k) + 2*c*(rpp.col(k) - rpp.col(not_k))/( (1 + b*rijpp)*(1 + b*rijpp)*rijpp );
    r.col(k) = rpp.col(k) + D*Fpp*dt + randn<vec>(2)*sqrt(dt);
    rij = norm(r.col(0)-r.col(1));
    F    = -2*a*w*r.col(k) + 2*c*(r.col(k) - r.col(not_k))/( (1 + b*rij)*(1 + b*rij)*rij );
    vec p = rpp.col(k) - r.col(k) - D*dt*F; 
    vec q = r.col(k) - rpp.col(k) - D*dt*Fpp;
    double qji = exp(- dot(p,p)/(4*D*dt)); 
    double qij = exp(- dot(q,q)/(4*D*dt));
    
    // computing new state function
    wf = exp(-0.5*a*w*( dot(r.col(0),r.col(0)) + dot(r.col(1),r.col(1)) ) + r12/(1 + b*r12) );
	
    // one iteration of Hastings-Metropolis theorem
    if ( wf*wf*qji/(wfpp*wfpp*qij ) > rand_double(gen) ) {
        rpp = r; wfpp = wf; rij = norm(r.col(0) - r.col(1) );
    }
    
    // sample energy
    double e = 1/rij + 0.5*w*w*(1-a*a)*( dot(rpp.col(0),rpp.col(0)) + dot(rpp.col(1),rpp.col(1)) )
            + 2*a*w + a*w*c*rij/pow(1 + rij*b,2) - c*(1+rij*c-b*b*rij*rij)/( rij*pow(1 + rij*b,4) );
    E += e/iterations;
}
\end{lstlisting}
\caption{Program for Hastings-Metropolis-style implementation of a quantum dot of two electrons. The variables \texttt{r}, \texttt{rpp} contain the positions at sequential steps, \texttt{F}, \texttt{Fpp} contain the quantity $\nabla \psi_T/\psi_T$ for sequential steps and \texttt{rij}, \texttt{rijpp} contain the distance between electrons at sequential steps. The program returns the energy with uncertainty, $\Var{\E{E}} = 10^{-10}$ and rejection ratio $r < 10^{-4}$ for the importance sampling.}
\end{table}
Interestingly, for $\omega = 1$ the program returns that the energy is $\E{E} = 3.00052\pm 10^{-5}$ after only $2^{20} \approx 10^6$ Monte-Carlo cycles. For reference, the exact result is 3.\\
\\
We want to generalize the program to more than two, $n>2$, electrons. This adds the complication of adding a Slater determinant. Further consequences are that the analytical expression of the energy becomes large. This is chiefly due to the dependence of $\alpha$ in the functions $\phi_{ij}$ in the Slater determinant and their complicated interactions. Therefore $\nabla^2 \psi_T/\psi_T$ becomes a complicated expression. However, note that for the trail state function the chain rule implies
 \[
\frac{\nabla_i^2 \psi_T}{\psi_T} = \frac{\nabla_i^2 \det D_\uparrow}{\det D_\uparrow} + \frac{\nabla_i^2 \det D_\downarrow}{\det D_\downarrow} + \frac{\nabla_i^2 \psi_C}{\psi_C} + 2\left( \frac{\nabla_i \det D_\uparrow}{\det D_\uparrow} + \frac{\nabla_i \det D_\downarrow}{\det D_\downarrow} \right) \cdot \frac{\nabla_i \psi_C}{\psi_C}
\]
We wrote functions which evaluated each of these terms after first differentiating each term using pen and paper. The exact details of these differentiations are of little interest to post-calculus-students, and relegate these to the appendix for your reference.\\
\\
However more interesting is perhaps discussing how to evaluate the Hastings-Metropolis tests for larger systems of electrons. It turns out that this adds some complications that are easily dealt with. Suppose now that $X \sim \mathrm{unif}(0,1)$ is a uniformly continuously distributed random variable on $(0,1]$. As we saw from theorem \ref{thm:metropolis} we have to evaluate the following inequality quickly and reliably at each Monte-Carlo cycle:
\begin{equation}
X < \frac{\pi_jq_{ji}}{\pi_i q_{ij}} = \frac{q_{ji}}{q_{ij}} \left( \frac{\det(D_\uparrow D_\downarrow)_j }{\det(D_\uparrow D_\downarrow)_i} \frac{(\psi_C)_j}{(\psi_C)_i} \right)^2 \label{eq:ineq}
\end{equation}
The trouble is that due to the exponentials inside the Jastrow factor, these terms will become unpractically large and overflow for large systems of electrons. The path taken in the implementation of this project was the following. Suppose $f(x)$ is some monotonous $\mathbb{R} \to \mathbb{R}$-function. Since $f$ is monotonous, that means by definition that $x < y$ if and only if $f(x) < f(y)$. Since the natural logarithm $\log$ is monotonous we can take $x = X$, and $y$ equal the right hand side of inequality \eqref{eq:ineq}. Hence inequality \eqref{eq:ineq} is true if and only if 
\[
\log(X) < \log q_{ji} - \log q_{ij} + 2\big(\log( \det(D_\uparrow D_\downarrow)_j) - \log( \det(D_\uparrow D_\downarrow)_i) + \log (\psi_C)_j - \log (\psi_C)_i \big)
\]
Since $X > 0$ the logarithm and exponential are reverse functions for the terms in question, and hence the evaluation of the Jastrow factors are reduced to magnitudes smaller than about 1000. Expressions to implement the Steepest decent method are also contained in the appendix.\\
\\
Care was taken to check that the program ran correctly. 
\begin{itemize}
\item[>]For two particles, there were exact results for the ground state energy. The program reproduced this benchmark, as becomes apparent in the results. This was verified in the case that the program was running with and without the slater determinant. In addition, for 6 and 12 particles the expected energy was compared to \cite{m._pedersen_lohne_ab_2011}. As will become evident in the discussion, there was satisfactory agreement.
\item[>]For two particles, there were analytical expressions available for the trail energy, the Laplacians of both the harmonic oscillator basis states and the Jastrow factor. There were also analytical expressions for the gradients of the same functions. When a Slater determinant was introduced, the program was individually checked against all four of these, and the program passed millions of iterations to check all of these without a glitch.
\item[>]For two particles there were analytical expressions for the derivatives required for the steepest decent method. After a Slater determinant was introduced, the program was checked against both of these for millions of iterations, there were no discrepancies.
\item[>] It was verified that when the perturbations were switched off, we recovered the unperturbed energies for $n=2,6,12$. The simulated values were $2.0002\omega, 10,0031\omega$ and $28.0002\omega$ respectively.
\item[>]The program passed over 100 tests of tabulated values. A reference table of expected values were provided by the course administration on Piazza, for any number of particles, and chosen frequencies $\omega$. The program was checked against this table, and as will become evident in the results, the program produced reasonable results for all of these tests. Since there were 65 observables, this represented 65 tests of the program. There was also another table of values containing suggestions for variational parameters, this table contained 40 values. The program reproduced all these values satisfactorily.
\end{itemize}
For your reference, the source code is available from the github URL provided at the heading of this report.
\section*{\uppercase{Results and discussion}}
\begin{table}
\center
\begin{tabular}{c c c c c c}
$n$ &$\omega$& $\E{E}$						& $\E{K}$						& $\E{V}$							& $\E{r_{ij}}$							\\
\hline\\
	&1.00 	& $3.0057\pm 8\cdot 10^{-4}$	& $0.9217\pm 1\cdot 10^{-3}$& $2.0840\pm 2\cdot 10^{-2}$	& $1.6318\pm 3\cdot 10^{-2}$\\
	&0.50 	& $1.6595\pm 1\cdot 10^{-3}$ 	& $0.4521\pm 6\cdot 10^{-2}$ 	& $1.2073\pm 1\cdot 10^{-2}$		& $2.4424\pm 1\cdot 10^{-1}$\\
2	&0.10	& $0.4588\pm 6\cdot 10^{-3}$ 	& $0.0984\pm 1\cdot 10^{-3}$ 	& $0.3603\pm 6\cdot 10^{-3}$		& $5.4414\pm 1\cdot 10^{-1}$\\
	&0.05	& $0.2621\pm 6\cdot 10^{-4}$	& $0.0472\pm 4\cdot 10^{-4}$ 	& $0.2148\pm 8\cdot 10^{-4}$		& $8.6576\pm 6\cdot 10^{-2}$\\
	&0.01	& $0.0874\pm 3\cdot 10^{-3}$	& $0.0103\pm 4\cdot 10^{-3}$ 	& $0.0770\pm 4\cdot 10^{-3}$		& $18.740\pm 1\cdot 10^{-0}$\\
	\\
	&1.00 	& $20.231\pm 9\cdot 10^{-3}$ 	& $3.9071\pm 7\cdot 10^{-2}$ 	& $16.320\pm 6\cdot 10^{-2}$		& $2.1835\pm 3\cdot 10^{-2}$\\
	&0.50 	& $11.808\pm 1\cdot 10^{-2}$ 	& $1.7694\pm 4\cdot 10^{-2}$ 	& $10.038\pm 1\cdot 10^{-1}$		& $3.3861\pm 1\cdot 10^{-1}$\\
6	&0.10	& $3.5704\pm 2\cdot 10^{-3}$ 	& $0.3373\pm 3\cdot 10^{-3}$ 	& $3.2330\pm 3\cdot 10^{-3}$		& $9.2390\pm 1\cdot 10^{-1}$\\
	&0.05	& $2.1679\pm 2\cdot 10^{-3}$ 	& $0.1716\pm 7\cdot 10^{-3}$ 	& $2.0001\pm 5\cdot 10^{-3}$		& $14.058\pm 4\cdot 10^{-2}$\\
	&0.01	& $0.7177\pm 6\cdot 10^{-3}$ 	& $0.0049\pm 1\cdot 10^{-2}$ 	& $0.7127\pm 1\cdot 10^{-2}$		& $32.747\pm 8\cdot 10^{-1}$\\
	\\
	&1.00 	& $65.881\pm 2\cdot 10^{-2}$ 	& $9.4946\pm 7\cdot 10^{-2}$ 	& $56.387\pm 2\cdot 10^{-2}$		& $2.6842\pm 7\cdot 10^{-3}$\\
	&0.50 	& $39.250\pm 3\cdot 10^{-2}$ 	& $4.3144\pm 9\cdot 10^{-2}$ 	& $34.936\pm 8\cdot 10^{-2}$		& $4.1283\pm 1\cdot 10^{-1}$\\
12	&0.10	& $12.270\pm 2\cdot 10^{-3}$ 	& $0.8171\pm 1\cdot 10^{-2}$ 	& $11.503\pm 1\cdot 10^{-2}$		& $11.369\pm 3\cdot 10^{-2}$\\
	&0.05	& $7.6022\pm 9\cdot 10^{-3}$ 	& $0.4222\pm 1\cdot 10^{-2}$ 	& $7.1799\pm 1\cdot 10^{-2}$		& $17.734\pm 4\cdot 10^{-2}$\\
	&0.01	& $2.5671\pm 3\cdot 10^{-3}$ 	& $0.0913\pm 3\cdot 10^{-3}$ 	& $2.4757\pm 4\cdot 10^{-3}$		& $44.243\pm 2\cdot 10^{-1}$\\
	\\
	&1.00 	& $156.25\pm 4\cdot 10^{-2}$ 	& $18.124\pm 3\cdot 10^{-1}$ 	& $138.13\pm 3\cdot 10^{-2}$		& $3.1754\pm 4\cdot 10^{-2}$\\
	&0.50 	& $93.918\pm 6\cdot 10^{-2}$ 	& $8.1185\pm 2\cdot 10^{-1}$ 	& $85.863\pm 1\cdot 10^{-1}$		& $4.8527\pm 1\cdot 10^{-1}$\\
20	&0.10 	&$30.237\pm 1 \cdot 10^{-2}$ 	&$1.6442\pm 1 \cdot 10^{-2}$	& $28.592\pm 8\cdot 10^{-2}$		& $13.807\pm 1 \cdot 10^{-2}$\\
	&0.05	&$18.763\pm 3 \cdot 10^{-2}$	&$0.8454\pm 5 \cdot 10^{-2}$	& $17.917\pm 2\cdot 10^{-2}$		& $20.942\pm 5 \cdot 10^{-2}$\\
	&0.01	&$6.4086\pm 5 \cdot 10^{-2}$ 	&$0.1699\pm 6 \cdot 10^{-2}$	& $6.2386\pm 2\cdot 10^{-3}$		& $52.237\pm 3 \cdot 10^{-1}$\\
\end{tabular}
\caption{Estimates of expected value for the state function specified by equation \eqref{eq:wf}. The error bars given represent one standard deviation from the mean. $\E{E}$, $\E{K}$ and $\E{V}$ represent the expectation of ground state- , kinetic- and potential energy respectively, whilst $\E{r_{ij}}$ represent the mean distance between any two of the $n$ electrons at the frequency $\omega$.}\label{tbl:observables}
\end{table}
\begin{table}
\center
\begin{tabular}{c c c c c c}
$n$ &$\omega$& $\alpha$		& $\beta$		& $\Delta t$	& Acceptance \\
\hline\\
	&1.00 	& $0.988664$	& $0.397451$	& $0.0010$		& $>99.999\%$\\
	&0.50 	& $0.942741$ 	& $0.358363$ 	& $0.0025$		& $>99.999\%$\\
2	&0.10	& $0.952742$ 	& $0.354292$ 	& $0.0050$		& $>99.999\%$\\
	&0.05	& $0.913124$	& $0.236505$ 	& $0.0750$		& $>99.999\%$\\
	&0.01	& $0.911693$	& $0.203917$ 	& $0.0100$		& $>99.999\%$\\
	\\       
	&1.00 	& $1.040159$ 	& $0.469328$ 	& $0.0010$		& $>99.999\%$\\
	&0.50 	& $0.929001$ 	& $0.395588$ 	& $0.0025$		& $>99.999\%$\\
6	&0.10	& $0.831105$ 	& $0.211448$ 	& $0.0500$		& $>99.999\%$\\
	&0.05	& $0.851754$ 	& $0.155691$ 	& $0.0750$		& $>99.999\%$\\
	&0.01	& $0.839981$ 	& $0.106890$ 	& $0.0100$		& $>99.999\%$\\
	\\
	&1.00 	& $1.059616$ 	& $0.475495$ 	& $0.0050$		& $>99.999\%$\\
	&0.50 	& $0.945597$ 	& $0.414537$ 	& $0.0025$		& $>99.999\%$\\
12	&0.10	& $0.841061$ 	& $0.217988$ 	& $0.0500$		& $>99.999\%$\\
	&0.05	& $0.842746$ 	& $0.153921$ 	& $0.0750$		& $>99.999\%$\\
	&0.01	& $0.835881$ 	& $0.092328$ 	& $0.5000$		& $>99.999\%$\\
	\\
	&1.00 	& $1.079881$ 	& $0.475432$ 	& $0.0010$		& $>99.999\%$\\
	&0.50 	& $0.944121$ 	& $0.424048$ 	& $0.0025$		& $>99.999\%$\\
20	&0.10 	& $0.857020$ 	& $0.200731$	& $0.1000$		& $>99.999\%$\\
	&0.05	& $0.834301$	& $0.153331$	& $0.0750$		& $>99.999\%$\\
	&0.01	& $0.835321$ 	& $0.092253$	& $0.5000$		& $>99.999\%$\\
\end{tabular} 
\caption{Suitable choice of paramaters. Here $\alpha$ and $\beta$ refer to the variational parameters of the Slater-Jastrow-type state function chosen whilst $\Delta t$ give the variance of the positions $r^{(i)}_k$ for all $i,k \in \mathbb{N}$. Acceptance refers to the ratio of the accepted values of $r^{(i)}_k$ to the total number of Monte-Carlo cycles. Typically the first non-nine decimal place was the 5th or 6th digit.}\label{tbl:params}
\end{table}
Our results encompassed both physics and mathematics results. We start by presenting the physics results. We obtained estimates for the expected kinetic-, potential-, and total energy for $n=2,6,12,20$ electrons, as well as the expected interparticle distance for $\omega = 1, 0.5, 0.1, 0.05, 0.01$ with confidence intervals for all estimates using the blocking method. Please see table \ref{tbl:observables}. Using the method of steepest decent we were able to find parameters $\alpha,\beta$ such that $\psi(\vec{r}_1,\dots,\vec{r}_n;\alpha,\beta)$ minimize the local energy. We present these along with suitable choice of values of $\Delta t$ since we found that the choice of $\Delta t$ was just as important as the variational parameters. We also see that the acceptance for all simulations were larger than $99.9999\%$. Please see table \ref{tbl:params} for full details. Next we completed a performance analysis in the case $n=6$ electrons in order to study the effect of vectorization and parallelization on the scheme. Since the majority of the work is the evaluation of functions and updating the variables in memory, almost all of the work could be parallelized. In fact, suppose you regard $f$ as the random variable which represent the fraction of time that was perfectly parallelizable relative to the whole, then we found that
\[
\E{f} = 0.9999988 \qquad \text{and} \qquad \Var{f} = 6.05 \cdot 10^{-14}.
\]
We know by Jensen inequality that if $X$ is a random variable and $f : \mathbb{R} \to \mathbb{R}$ is a convex function $(\dagger)$, then $f(\E{X}) \leq \E{f(X)}$ with equality if and only if $X$ has zero variance. It turns out that $x \mapsto x^{-1}$ is a convex function, as you can check, and therefore
\begin{equation}
\E{\text{speedup}} = \E{\frac{1}{(1-f) - f/p}} \stackrel{(\dagger)}{>} \frac{1}{\E{(1-f) - f/p}} = \frac{1}{(1-\E{f}) - \E{f}/p} \label{eq:speedup}
\end{equation}
Assuming that $f$ is a continuous random variable with continuous probability density function, then limits commute with expectation by the Dominated convergence theorem (DCT) , i.e 
\begin{align*}
\E{\text{max speedup}} &= \E{ \lim_{p \to \infty} \frac{1}{(1-f) - f/p}} \stackrel{\text{(DCT)}}{=} \lim_{p \to \infty} \E{ \frac{1}{(1-f) - f/p}} \stackrel{\eqref{eq:speedup}}{>} \lim_{p \to \infty}\frac{1}{\E{(1-f) - f/p}} \\
&= \lim_{p \to \infty} \frac{1}{(1-\E{f}) - \E{f}/p} = \frac{1}{1 - \E{f}} = 833333.
\end{align*}
Unfortunately, computing the variance of the maximal speed up is more complicated.\\
\\
The code was vectorized to investigate the implied performance increase. Surprisingly, we found no significant mean increase in runtimes by vectorization at $10^6$ Monte-Carlo cycles. Suppose the vectorized runtimes are $X_1,\cdots,X_n$ with mean $\mu_x$ and the serial runtimes are $Y_1,\cdots,Y_n$ with mean $\mu_y$. If all the observation are independent, normally distributed with some variance $\sigma^2$, then we are in the perfect setting to use the Analysis of Variance method to check if $\mu_x = \mu_y$ \parencite{devore_modern_2012}. The result was that Analysis of Variance found no significant difference between $\mu_x$ and $\mu_y$ ($p = 0.7634$). In fact $\sigma = 24.18$ and if $\mu_x = \mu_y = \mu$, then $\mu = 75.1218$ seconds on 18 degrees of freedom. Please see table \ref{tbl:anova} for full details
\begin{table}[!h]
\begin{center}
\begin{tabular}{l r r r r r}
          &df&  Sum Sq& Mean Sq& F value& Pr(>F)\\
          \hline
          \\
Vectorization    & 1&    54.6&   54.64&  0.0934& 0.7634\\
Residuals &18& 10528.1&  584.9&0
\end{tabular}
\begin{lstlisting}
\end{lstlisting}
\caption{By Analysis of variance, we found no significant difference between the runtimes of the vectorized and serial code, given that the 20 runtimes were independent, normally distributed with identical variance on 18 degrees of freedom. } \label{tbl:anova}
\end{center}
\end{table}


%I Give a critical discussion of your work and place it in the
%correct context.
%I Relate your work to other calculations/studies
%I An eventual reader should be able to reproduce your
%calculations if she/he wants to do so. All input variables
%should be properly explained.
%I Make sure that figures and tables should contain enough
%information in their captions, axis labels etc so that an
%eventual reader can gain a first impression of your work by
%studying figures and tables only.

The remaining results develop the mathematics of the blocking method. Let's start with a soft result on which we will build more structure.
\begin{prop}
Suppose $X_1,\cdots, X_m$ are $m = 2^d$ identically distributed random variables with stationary non-negative covariances. Let $\overline X_m = (1/m)\sum_{j=1}^m X_j$ denote their average and $X^{(j)}_i$ be a random variable subject to $j$ blocking transformations. Then
\begin{equation}
\varepsilon_k = \Var{\overline X_m} - \Var{X^{(k)}_i} = \frac{2}{n^{(k)}} \sum_{t=1}^{n^{(k)}-1} \left( 1 - \frac{t}{n^{(k)}} \right) \gamma_t^{(k)} \label{eq:epsilon}
\end{equation}
and
\begin{equation}
\gamma_{j}^{(k)} = \ccases{ \frac{1}{2}\gamma_{2j}^{(k-1)} + \frac{1}{2}\gamma_{2j+1}^{(k-1)} & \qquad \text{if $j = 0$} \\
\frac{1}{4}\gamma_{2j-1}^{(k-1)} + \frac{1}{2}\gamma_{2j}^{(k-1)} + \frac{1}{4}\gamma_{2j+1}^{(k-1)} & \qquad \text{if $j > 0$}
} \label{eq:T}
\end{equation}
\end{prop}
\begin{proof}
See \cite{flyvbjerg_error_1989}.
\end{proof}
Next let's make a couple of simple observations
\begin{lemma} Suppose $m = 2^d$, $d > 1$ denotes the number of observations and $k$ denote the number of blocking transformations. Then
\[
n^{(k)} = 2^{-k}m \qquad \text{and} \qquad k < \log_2m \qquad \text{for all} \qquad k<d.
\]\label{lemma:k}
\end{lemma}
\begin{proof}
We proof the first part by induction. Suppose $k=0$. By definition, $n^{(0)} = m$ and so
\[
n^{(0)} = m = 2^{-0} m,
\]
Suppose now that there exist some $k \in \{0,1,2,\cdots\}$ such that $n^{(k)} = 2^{-k}m$ and $n^{(k+1)}$ exist, this is possible since $d > 1$ by hypothesis. By construction each blocking transformation halve the number of observations, so $n^{(k+1)} = n^{(k)}/2$ and therefore
\[
n^{(k+1)} = \frac{n^{(k)}}{2} = \frac{2^{-k}m}{2} = 2^{-(k+1)}m.
\]
To see the last observation is true, just observe that if $k < d$ and $m = 2^d$, then $k < d = d\log_2 2 = \log_2 m$.
\end{proof}
\begin{lemma} Suppose $\gamma_t^{(k)}$ denote the correlation between two observations $X_i$ and $X_j$ such that $|i-j| = t$. Suppose also that $\gamma_t^{(k)} \geq 0$ for all $t$. Then 
\[
\gamma_t^{(k)} \leq \gamma_0^{(k)} \qquad \text{for all} \qquad t \geq 0.
\]
\end{lemma}
\begin{proof}
We will need the formula $| \cov (X,Y) |^2 \leq \cov (X,X) \cov (Y,Y)$ $(*)$ which is proven in the appendix. Write
\begin{align*}
(\gamma_{t}^{(k)})^2 &= |\gamma^{(k)}_t|^2 = |\gamma_{ij}^{(k)}|^2 = | \cov (X^{(k)}_i,X^{(k)}_j) |^2 \\
&\stackrel{(*)}{\leq} \cov (X^{(k)}_i,X^{(k)}_i) \cov (X^{(k)}_j,X^{(k)}_j) = \gamma_{ii}^{(k)}\gamma_{jj}^{(k)} = (\gamma_0^{(k)})^2
\end{align*}
Since the function $f(\gamma) = \gamma^2$ is increasing on $[0,\infty)$, we know $\gamma_t^{(k)} \geq \gamma_0^{(k)}$ follows.
\end{proof}
I doubt that the next proposition alone is enough to imply the blocking method, but its strength become evident once we consider its corollaries.
\begin{prop}
Suppose $\varepsilon_k$ denotes the error after $k$ blocking iterations and $\gamma^{(k)}_t \geq 0$ for all $k,t \in \mathbb{N}$. Let $\{\varepsilon_k\}_{  k=1}^d$ denote the sequence of errors. Then the rate of change of the sequence is
\begin{equation}
\varepsilon_k - \varepsilon_{  k+1} = \frac{\gamma_1^{(k)} }{n^{(k)}} \label{eq:rate}
\end{equation}
\label{prop:diff}
\end{prop}
\begin{proof}
It remains to prove formula \eqref{eq:rate}. We will manipulate sums and will be interested in which function $f(t)$ appear in the terms $\gamma^{(k)}_{f(t)}$ inside the summation. Define a functional $S_{f(j)} : \{2x-1,2x,2x+1\} \to \mathbb{R}$ by: 
\[S_{f(j)} \equiv \frac{1}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t}{n^{(k)}} \right) \gamma^{(k)}_{f(t)}. \]
That means, using $n^{(k+1)} = n^{(k)}/2$ , we can rewrite $\varepsilon_{  k+1}$ as
\begin{align}
\varepsilon_{  k+1} &\stackrel{\eqref{eq:epsilon}}{=} \frac{2}{n^{(k+1)}} \sum_{t=1}^{n^{(k+1)}-1} \left( 1 - \frac{t}{n^{(k+1)}} \right) (\gamma_{k+1})_t \stackrel{\eqref{eq:T}}{=} \frac{1}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t}{n^{(k)}} \right) \left( \gamma^{(k)}_{2t - 1} + 2\gamma^{(k)}_{2t} + \gamma^{(k)}_{2t + 1}\right) \nonumber \\
&= S_{2j-1} + 2S_{2j} + S_{2j+1} = 2(S_{2j-1} + S_{2j}) + S_{2j+1} - S_{2j-1} \label{eq:epsilonkpp}
\end{align}
where we added and subtracted $S_{2j - 1}$ in the last step. Write out the sums explicitly to obtain:
\begin{align}
S_{2j-1} &= \frac{1}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t}{n^{(k)}} \right) \gamma^{(k)}_{2t - 1}  = \frac{1}{n^{(k)}}\sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t-1}{n^{(k)}}\right) \gamma^{(k)}_{2t - 1} - \frac{1}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2-1} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} \label{eq:sumB}
\\
S_{2j+1} &= \frac{1}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t}{n^{(k)}} \right) \gamma^{(k)}_{2t + 1}  = \frac{1}{n^{(k)}} \left[ \frac{2}{n^{(k)}} \gamma^{(k)}_{3} + \frac{4}{n^{(k)}} \gamma^{(k)}_{5} + \cdots + \frac{n^{(k)} - 2}{n^{(k)}} \gamma^{(k)}_{n^{k} - 1}\right]\nonumber
 \\
 &= \frac{1}{n^{(k)}}\sum_{t=2}^{n^{k}/2} \left( 1 - \frac{2(t-1)}{n^{(k)}} \right) \gamma^{(k)}_{2t-1} = \frac{1}{n^{(k)}} \sum_{t=2}^{n^{(k)}/2} \left( 1 - \frac{2t-1}{n^{(k)}} \right) \gamma^{(k)}_{2t - 1} + \frac{1}{n^{(k)}}\sum_{t=2}^{n^{(k)}/2} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1}  \nonumber
\end{align}
Notice that if thse terms are subtracted we obtain
\begin{equation}
S_{2j+1} - S_{2j-1} = \frac{2}{n^{(k)}} \sum_{t=2}^{n^{(k)}/2} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} - \frac{1}{n^{(k)}}\left( 1 - \frac{1}{n^{(k)}} \right) \gamma^{(k)}_{1} \label{eq:differencesum}
\end{equation}
Lets also investigate the following quantity
\begin{align}
n^{(k)}(S_{2j-1} + S_{2j}) \stackrel{\eqref{eq:sumB} }{=} \sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t-1}{n^{(k)}}\right) \gamma^{(k)}_{2t - 1} -  \sum_{t=1}^{n^{(k)}/2-1} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} + \sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t}{n^{(k)}}\right) \gamma^{(k)}_{2t}\nonumber\\
=\left[ \left(1 + \frac{1}{n^{(k)}}\right)\gamma^{(k)}_1 + \left(1 + \frac{2}{n^{(k)}}\right)\gamma^{(k)}_2  + \cdots + \left(1 + \frac{n^{(k)} - 2}{n^{(k)}} \right)\gamma^{(k)}_{n^{(k)} -2 }\right]-  \sum_{t=1}^{n^{(k)}/2-1} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} \nonumber\\
= \sum_{t=1}^{n^{(k)} -2 } \left( 1 - \frac{t}{n^{(k)}} \right)\gamma^{(k)}_{t}-  \sum_{t=1}^{n^{(k)}/2-1} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} = \sum_{t=1}^{n^{(k)} -1 } \left( 1 - \frac{t}{n^{(k)}} \right)\gamma^{(k)}_{t} -  \sum_{t=1}^{n^{(k)}/2} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} \label{eq:nsum}
\end{align}
In the last equality we added and subtracted $(1-t/n^{(k)})\gamma^{(k)}_{t}$ and in addition used that for $t = n^{(k)} - 1$, $(1-t/n^{(k)})\gamma^{(k)}_{t} = 1/n^{(k)}\gamma^{(k)}_{n^{(k)} -1}$. This means that if we consider
\begin{align}
2(S_{2j-1} + S_{2j}) \stackrel{\eqref{eq:nsum}}{=} \frac{2}{n^{(k)}} \left[ \sum_{t=1}^{n^{(k)} -1 } \left( 1 - \frac{t}{n^{(k)}} \right)\gamma^{(k)}_{t} -  \sum_{t=1}^{n^{(k)}/2} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} \right] = \varepsilon_k -\frac{2}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} \label{eq:sumsum}
\end{align}
Now substitute \eqref{eq:differencesum} and \eqref{eq:sumsum} into \eqref{eq:epsilonkpp}
\begin{align*}
\varepsilon_{  k+1} &\stackrel{\eqref{eq:epsilonkpp}}{=} 2(S_{2j-1} + S_{2j}) + S_{2j+1} - S_{2j-1} \\
&\stackrel{\eqref{eq:sumsum}}{=} \varepsilon_k -\frac{2}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1}  + \frac{2}{n^{(k)}} \sum_{t=2}^{n^{(k)}/2} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} - \frac{1}{n^{(k)}}\left( 1 - \frac{1}{n^{(k)}} \right) \gamma^{(k)}_{1}\\
&= \varepsilon_k - \frac{1}{n^{(k)}}\frac{\gamma^{(k}}{n^{(k)}}  - \frac{\gamma^{(k)}_1 }{n^{(k)}} + \frac{1}{n^{(k)}}\frac{\gamma^{(k}}{n^{(k)}} = \varepsilon_k - \frac{\gamma^{(k)}_1 }{n^{(k)}}
\end{align*}
Subtract $\varepsilon_{  k+1} - \gamma^{(k)}_1/n^{(k)}$ from each side of the equation, and the proposition follows.
\end{proof}
The proposition says that the rate of convergence of the method is only dependant on the correlation between sequential observations $X_i^{(k)}$, and not the full correlation structure. But the following two corollaries show that the proposition prove the expected behaviour of the blocking method.
\begin{corollary}
Assume $\gamma^{(k)}_t \geq 0$ for all $t$ then
\begin{itemize}
\item if $\gamma^{(k)}_1 > 0$ for all $j \leq k \leq i$, the sequence of errors $\varepsilon_k$ is strictly decreasing for all $j \leq k \leq i$.
\item if $\gamma^{(k)}_1 \geq 0$ for all $j \leq k \leq i$, the sequence of errors $\varepsilon_k$ is decreasing for all $j \leq k \leq i$.
\item if there exist some $k$ such that $\gamma^{(k)}_1 = 0$, then the sequence of errors $\varepsilon_k$ become constant.
\end{itemize}
\end{corollary}
\begin{proof}
We prove the first part of the hypothesis. Assume the hypothesis is true and $\gamma^{(k)}_1 > 0$ for all $j \leq k \leq i$. That means proposition \ref{prop:diff} is true. Suppose $u>v$ and notice that since $n^{(k)} > 0$ and $\gamma^{(k)}_1 > 0$ is by hypothesis, then the sum of such terms must be positive, and therefore
\begin{align*}
0 < \sum_{k = v}^{u-1} \frac{\gamma^{(k)}_1 }{n^{(k)}} &= \frac{\gamma^{(v)}_1}{n^{(v)}} + \frac{\gamma^{(v+1)}_1}{n^{(v+1)}} + \cdots + \frac{\gamma^{(u-1)}_1}{n^{(u-1)}} \stackrel{\eqref{eq:rate}}{=} (\varepsilon_v - \varepsilon_{v+1}) + (\varepsilon_{v+1} - \varepsilon_{v+2}) + \cdots + (\varepsilon_{u-1} - \varepsilon_{u}) \\
&= \varepsilon_v - \varepsilon_{u},
\end{align*}
since every term of the sum cancel except from the first and the last. Now, by adding $\varepsilon_u$ to each side of that strict inequality, the first part is proven. To obtain the rest of the corollary, just replace the strict inequality with an inequality or equality.
\end{proof}

\begin{corollary}
If the blocked variables become independent, for some number of blocking transformation, then the sequence of errors $\varepsilon_k$ become constant.
\end{corollary}
\begin{proof}
Use the previous corollary, and notice that $\gamma^{(k)}_1 = 0$ if $X_i,X_{i+1}$ are independent for all $0 \leq i,j \leq 2^{-k}m$.
\end{proof}

\begin{lemma}
Suppose $j$ and $k$ are positive natural number and the sample size $m \geq 2^k(j-1) + 2^{k+1} - 1$, then $k < \log_2 m$ and
\begin{align}
\gamma_{j}^{(k)} &= 2^{-2k} \Big[ \gamma_{2^k(j-1)+1}^{(0)} + 2\gamma_{2^k(j-1)+2}^{(0)} +3\gamma_{2^k(j-1)+3}^{(0)} + \cdots + 2^k\gamma_{2^k(j-1)+2^k}^{(0)} \nonumber \\
&+ (2^k -1) \gamma_{2^k(j-1)+2^k + 1}^{(0)} + (2^k -2) \gamma_{2^k(j-1)+2^k + 2}^{(0)} + \cdots + \gamma_{2^k(j-1)+2^{k+1} - 1}^{(0)} \Big] . \label{eq:hypothesis}
\end{align}
\end{lemma}
\begin{proof}
We first show that $k < \log_2 m$. Fix $j$ and $k$ such that $m \geq 2^k(j-1) + 2^{k+1} - 1$, then
\[
m \geq \undercbrace{2^k(j-1)}_{\geq 0} + \undercbrace{2^{k+1} - 1}_{\geq 2^k +1} \geq 2^k + 1 \qquad \text{only if} \qquad \log_2m \geq \log_2(2^k + 1) > \log_2 2^k = k \log_2 2 = k
\]
We prove the rest of the lemma by induction. Fix $j$ such that $m \geq 2^1(j-1) + 2^{1+1} - 1$, in particular this ensures that if $k = 1$, then $m \geq 2j + 1$ and therefore $\gamma_{2j+1}^{(0)}$ exists. Define $M = \sup_{k \in \mathbb{N}} \{m \geq 2^k(j-1) + 2^{k+1} - 1\}$. Assume $k=1$ and write
\[
\gamma_{j}^{(1)} \stackrel{\eqref{eq:T}}{=} 2^{-2} \left( \gamma_{2j-1}^{(0)} + \gamma_{2j}^{(0)} + \gamma_{2j+1}^{(0)} \right)
\]
Assume now that there exist a positive natural number $k < M$ such that equation \eqref{eq:hypothesis} is true. This implies $k+1 \leq M$ and hence $\gamma_{2^k2j+2^k+2^k-1}^{(0)}$ exists, and we can write
\begin{align*} 
\gamma_{j}^{(k+1)} \stackrel{\eqref{eq:T}}{=} &2^{-2} (\gamma_{  k,2j-1} + 2\gamma_{  k,2j} + \gamma_{  k,2j+1}) \stackrel{\eqref{eq:hypothesis}}{=} \\
&2^{-2} 2^{-2k}\Big( \gamma_{0,2^k(2j-2)+1} + 2\gamma_{0,2^k(2j-2)+2} + \cdots + 2^k\gamma_{0,2^k(2j-2)+2^k} \\
&+ (2^k-1)\gamma_{0,2^k(2j-2)+2^k+1} + \cdots + \gamma_{0,2^k(2j-2)+2^k+2^k-1}
\Big) + \\
&2^{-2} 2^{-2k}\Big( 2\gamma_{0,2^k(2j-1)+1} + 4\gamma_{0,2^k(2j-1)+2} + \cdots + 2\,2^k\gamma_{0,2^k(2j-1)+2^k} \\
&+ 2(2^k-1)\gamma_{0,2^k(2j-1)+2^k+1} + \cdots + 2\gamma_{0,2^k(2j-1)+2^k+2^k-1}
\Big) + \\
&2^{-2} 2^{-2k}\Big( \gamma_{0,2^k2j+1} + 2\gamma_{0,2^k2j+2} + \cdots + 2^k\gamma_{0,2^k2j +2^k} \\
&+ (2^k-1)\gamma_{0,2^k2j+2^k+1} + \cdots + \gamma_{0,2^k2j+2^k+2^k-1}
\Big).
\end{align*}
By using that $2^{-2} 2^{-2k} = 2^{-2(k+1)}$ and factoring $\gamma_{K}^{(0)}$ together for all $2^{k+1}(j-1) + 1 \leq K \leq 2^{k+1}(j-1) + 2^{k+2}-1$, the lemma follows.
\end{proof}
The next proposition is useful because combined with proposition \ref{prop:diff}, it shows how the convergence of the method is uniquely determined by the initial correlation structure.
\begin{prop}
Suppose $\big(\gamma_1^{(k)} , \gamma_2^{(k)}, \cdots, \gamma_{n_k}^{(k)}\big)$ denote the correlation structure at blocking iteration number $k$. Suppose $m \geq 2^{k+1} - 1$, then $k < \log_2 m$ and,
\[
2^{  2k} \gamma_1^{(k)} = \gamma_1^{(0)} + 2\gamma_2^{(0)} + 3 \gamma_3^{(0)} + \cdots +2^k \gamma_{2^k}^{(0)} + (2^k-1)\gamma_{2^k+1}^{(0)} + \cdots + \gamma_{2^{k+1}-1}^{(0)}
\] \label{prop:sequence}
\end{prop}
\begin{proof}
Use the previous lemma with $j=1$.
\end{proof}
\begin{theorem}[Blocking method]
Suppose $X_1,\cdots, X_m$ are $m = 2^d$ identically distributed random variables with stationary non-negative covariances. Let $\overline X_m = (1/m)\sum_{j=1}^m X_j$ denotes their average and $X^{(j)}_i$ be a random variable subject to $j$ blocking transformations. Then there exist a $k \in \mathbb{N}$ such that 
\[
k < \log_2 m \qquad \text{and} \qquad \Var{\overline X_m} - \Var{X^{(k)}_i} < \frac{8}{m}\Var{X^{(k)}_i} \qquad \text{for all} \qquad 1 \leq i \leq 2^{-k}m.
\]
\end{theorem}
\textit{Proof to come.} Did not have time to finish it before the exams. Furthermore, the proof appear to be more difficult than first anticipated. If there is interest in it, I can try to work out the details after the exams finish.\\
\\
This theorem says that after less than $\log_2 m$ blocking iterations, the difference between the variance of the mean and variance of the blocked variables, is essentially zero. This is the case since $m$ is typically many orders of magnitude larger than $\Var{X^{(k)}_i}$.\\
\\
We proceed to discuss the results in the order they were presented. Comparing the expected energies of the ground state that we calculated, with references such as CCSD done by \cite{m._pedersen_lohne_ab_2011} for $\omega = 1$ and $n = 2,6$ and 12, we see that our errors are roughly $0.1\%$. The CCSD computations found $\E{E} = 3.000282 $ and $20.189900$ and $65.789460$ respectively, whilst we obtained $\E{E} = 3.0057\pm 8\cdot 10^{-4} $ and $ 20.231\pm 9\cdot 10^{-3} $ and $ 65.881\pm 2\cdot 10^{-2}$ respectively. The discrepancy could be due to multiple causes. For example, the test state function, although arguably reasonable, is not exact. The variational parameters is only specified up to some finite number of digits. Moreover, it became apparent that the parameter $\Delta t$ was of utmost importance for stability and actually for the value of the final estimates. It turned out that it was possible to adjust this value to coarsely adjust the estimated values. This was also true about the estimates obtained by steepest decent. This is unfortunate, because we have no reference to which we know how to fix $\Delta t$.\\
\\
Surprisingly the value of the parameters to which the steepest decent algorithm converged, appeared to depend on the value of $\Delta t$. However the dependence was much less systematic than for the expectation values. This could also be contributing factors to the accuracy of our estimates. As we saw, there are several ways to optimize the steepest decent method, we proposed to maximize $f(\vec{r}_{n} - \zeta_{n} \nabla f(\vec{r}_{n}))$ because intuitively it ensures that each step moves as as close as possible to the minimum. The problem with this is however, that it works intuitively, but there were problems in practise. If we instead approximate the expected energy by a quadratic form, it is straight forward to implement the conjugate gradient method, which only requires two steps for convergence \parencite{press_numerical_2007}. This would only be useful if the expected energy is an approximate quadratic form to begin with.
\\
\\
But there were more issues with the optimization with the steepest decent method: The stability of the implementation was reasonably good, but not good enough to run without supervision. Every once in a while the method diverged without warning. However, it seems reasonable that we should not only blame this on the optimization method: In fact the state function does not remain a probability distribution for all values of $\alpha,\beta$. For example, it is easy to choose starting variational parameters such that the expected energy is no longer an expectation value for any given starting configuration of the electrons. In this way, the function at hand also poses some complications. It could be useful to investigate whether it is possible to improve stability by also minimizing with respect to some other quantities, such as the variance of the energy for example. It would be interesting to investigate if some modern stochastic minimization method is better suited for the task. In statistics, we use maximum likelihood estimation using Newton-Raphson method.
\\
\\
Our investigation into performance gains due to parallelization showed that the speed ups are excellent. The proportion of the workload that was parallelizable was almost exactly 1. This number would probably increase if the number of Monte Carlo cycles increase because then a larger proportion of the time would be spent on sampling, and the overhead would stay almost exactly the same. So although 800 000 was a lower bound on the expected value of the maximal speed up, this estimate is potentially unbounded if the number of Monte-Carlo cycles increase. We saw that it was possible to estimate this with Jensen equality. As we stated, Jensen inequality becomes an equality in the case that $f$ has zero variance. In our case, the variance was $10^{-14}$, so it is possible that the expected value of the estimate is not only a lower bound, but close to the exact value. Unfortunately, estimating the variance of the estimate probably require numerics. I am unable of any analytical approach, but with bootstrapping it is straight forward to make such estimates. This pose prospects for further work. Also surprisingly we saw that there was no effect of vectorization of the serial code.\\
\\
The paper by Flyvbjerg and Petersen possibly give an excellent introduction for applications \parencite[461-466]{flyvbjerg_error_1989}. However, mathematically interested readers may be put off by the crude (and wrong) mathematical treatment. This was motivational, along with that a brief search for proof of the Blocking method returned nothing relevant. The new proofs probably deserve some refinement when there is more time, but that said, the two most important results of the new theory were propositions \ref{prop:diff} and \ref{prop:sequence}. The first proposition says that the errors of the blocking method will monotonously decrease until the blocked variables become independent. At this point the error become constant. However on first readings it may seem we are able to use equation \eqref{eq:epsilon} to fix this constant to zero. If this turns out to be correct, this proves the behaviour of the blocking method, and may suffice for any tutution of the blocking method. However, I believe there is more to be had from the theory. The proposition also says that at blocking iterations number $k$ the convergence rate of the method only sees the correlations between subsequent observations. To new readers, this may be startling. But my intuition is that the stability of the method may be much attributed to this: If the dependence of the correlation was more complicated, it is also reasonable to expect that the behaviour of the convergence was more complicated. The second proposition \ref{prop:sequence} connect proposition \ref{prop:diff} to the initial correlation structure. This correlation structure is available before blocking starts and, as we saw from proposition \ref{prop:diff} contain all information about the convergence. Therefore it should be possible do more work into finding the optimal number of blocking iterations. \cite{flyvbjerg_error_1989} understand the value of this, and dedicate an entire section of their paper to try to present workaround. There seems there are prospects for further work in all of these areas. Finally, we see that proposition \ref{prop:sequence} is the convolution of the initial auto-correlation structure with the Smarandache Crescendo Pyramidal sequence:
\[
\begin{matrix}
&&&&1&\\
&&&1&2&1\\
&&1&2&3&2&1\\
&1&2&3&4&3&2&1\\
&&&&\vdots
\end{matrix}
\]
It could be possible to use some of the existing research on the sequence and simplify the proposition further. This is interesting since it contains all the information about the convergence, and our ability to understand influence further developments.
\section*{\uppercase{Conclusion and perspectives}}
Our main finding are propositions \ref{prop:diff} and \ref{prop:sequence}. We conjecture that together they imply a more satisfying proof of the Blocking method than the one provided in this text. Though as they stand, the corollaries of proposition \ref{prop:diff} seem to prove the behaviour of the Blocking method behaviour if we are able to fix the constant which arises in the first corollary. Here more research is needed. There were not enough time to work out all the details before the exam. For the same reason it was not possible to find the time to complete the proof of the proposed upper bound of the blocking method. The proof seemed more difficult than first anticipated, but seems to be within reach since propositions \ref{prop:diff} and \ref{prop:sequence} together seem to make all the information about the convergence of the Blocking method available prior to Blocking begins.\\
\\
Our physics results were exciting, we made tables which characterized the behaviour of the electrons of the quantum dot for $n = 2,6,12$ and 20 electrons for a variety of values of $\omega$. Together, they tell us what the expected distance between the particles were, their, kinetic and potential energy. The values of the ground state energy had tolerances within $0.1\%$ with CCSD benchmarks. However, in this area, there is probably prospects for more work. In particular in regards to the occasional lack of stability and implementation of parameter optimization. Surprisingly we saw that there were no effect of vectorization, but the speed up due to parallelization was almost exactly 100\%\\
\\
Finally, we were asked to provide constructive ideas to the course administration such that the course is kept at an interactive level. At the time of writing, the execution of project 2 went relatively smoothly, and I am much impressed with the efforts, in particular of Håkon for helping me troubleshoot the code for many hours on one occasion. The only comment that I can make is that perhaps more information could have been available about the parameter $\Delta t$; this would certainly have helped me understand why I had singular Slater determinants whilst building the program. This concludes the report of project 2.
\section*{\uppercase{Appendix}}
\subsubsection*{\uppercase{analytical expressions}}
Definition of trail energy
\begin{equation}
E_T = \sum_{i=1}^N \left[ -\frac{1}{2} \frac{\nabla^2_i \psi_T}{\psi_T} + \frac{1}{2} \omega^2 r_i^2 + \sum_{j=i+1}^N \frac{1}{r_{ij}} \right]
\end{equation}
Chain rule on smooth $m-$manifolds
\[
 \qquad \frac{\nabla_i^2 \psi_T}{\psi_T} = \frac{\nabla_i^2 \psi_0}{\psi_0} + \frac{\nabla_i^2 \psi_C}{\psi_C} + 2\frac{\nabla_i \psi_C}{\psi_C}\frac{\nabla_i \psi_0}{\psi_0}
\]
Expressions to determine energy in the case $n=2$:
\[
\psi_T(\vec{r}_1, \vec{r}_2) = C(\psi_0 \psi_C)(\vec{r}_1, \vec{r}_2), \quad  \psi_0(\vec{r}_1, \vec{r}_2) = \exp \left(-\frac{1}{2} \alpha  \omega  \left( {{r}_1}^2+  {{r}_2}^2\right)\right),  \quad  \psi_C(\vec{r}_1, \vec{r}_2) =  \exp \left( \frac{\gamma r_{  12}}{\beta  r_{  12}+1} \right)
\]
\[
\frac{\nabla_i^2 \psi_0}{\psi_0} = \alpha^2 \omega^2 r_i^2 - 2 \alpha \omega, \qquad
\frac{\nabla_i^2 \psi_C}{\psi_C} = \frac{\gamma (1 + r_{ij} \gamma - \beta^2 r_{  12}^2)}{r_{  12}(1 + \beta r_{  12})^4}, \qquad \sum_{i=1}^2 \frac{\nabla_i \psi_C}{\psi_C}\cdot \frac{\nabla_i \psi_0}{\psi_0} = - \frac{\alpha \omega \gamma r_{ij}}{(1 + \beta r_{12})^2}
\]
\[
\frac{\nabla_i \psi_T}{\psi_T} = \vec{e}_1 \left( -\alpha \omega x_i + \frac{\gamma(x_i - x_j)}{(1 + \beta r_{ij})^2r_{ij}} \right) + \vec{e}_2 \left( - \alpha \omega y_i + \frac{\gamma(y_i-y_j)}{(1 + \beta r_{ij})^2r_{ij}} \right)
\]
Auxiliary expressions for steepest decent method in the case $n=2$:
\[
\frac{\partial}{\partial \theta_j} \E{E}(\theta_1,\theta_2,\cdots \theta_n) = 2 \E{\frac{E}{\psi_T} \frac{\partial \psi_T}{\partial \theta_j} } - 2\E{\frac{1}{\psi_T} \frac{\partial \psi_T}{\partial \theta_j} }\E{E}
\]
\[
\frac{1}{\psi_T}\pp \psi_T; \alpha; = - \frac{\omega}{2} \left( r_1^2 + r_2^2 \right) \qquad 
\frac{1}{\psi_T}\pp \psi_T; \beta; = - \gamma \left( \frac{r_{12}}{1 + \beta r_{12}} \right)^2
\]
For more than two particles the derivatives of the trail state function are
\[
\frac{\nabla_i \psi_T}{\psi_T} = \frac{\nabla_i \det D_\uparrow}{\det D_\uparrow} + \frac{\nabla_i \det D_\downarrow}{\det D_\downarrow} + \frac{\nabla_i \psi_C}{\psi_C}
\]
\[
\frac{\nabla_i^2 \psi_T}{\psi_T} = \frac{\nabla_i^2 \det D_\uparrow}{\det D_\uparrow} + \frac{\nabla_i^2 \det D_\downarrow}{\det D_\downarrow} + \frac{\nabla_i^2 \psi_C}{\psi_C} + 2\left( \frac{\nabla_i \det D_\uparrow}{\det D_\uparrow} + \frac{\nabla_i \det D_\downarrow}{\det D_\downarrow} \right) \cdot \frac{\nabla_i \psi_C}{\psi_C}
\]
Derivatives of determinants
\[
\frac{\nabla_i \det D_\uparrow}{\det D_\uparrow} = \sum_{i = 0}^{n/2-1} (D_\uparrow^{-1})_{ji} \nabla_i \phi_{j}(\vec{r}_i), \quad \frac{\nabla_i^2 \det D_\uparrow}{\det D_\uparrow} = \sum_{i = 0}^{n/2-1} (D_\uparrow^{-1})_{ji} \nabla_i^2 \phi_{j}(\vec{r}_i); \quad \text{for $D_\downarrow$ replace $\uparrow$ by $\downarrow$}
\]
\[
\nabla \phi_{  nx,ny} = \vec{e}_1 \left( 2n_x (\alpha \omega)^{1/2} \phi_{  nx-1,ny} - \alpha \omega x \phi_{  nx,ny} \right) + \vec{e}_2 \left( 2n_y(\alpha\omega)^{1/2} \phi_{  nx,ny-1} - \alpha \omega y \phi_{  nx,ny} \right)
\]
\begin{align*}
\nabla^2 \phi_{  nx,ny} &= 4 \alpha \omega \left( n_x (n_x - 1)\phi_{  nx-2,ny} + n_y(n_y - 1)\phi_{  nx,ny-2} \right)  -4 (\alpha \omega)^{3/2} \left( x n_x\phi_{  nx-1,ny} + yn_y \phi_{  nx,ny-1} \right) \\
&+ \alpha \omega \phi_{  nx,ny}\left( \alpha\omega r^2 - 2 \right)
\end{align*}
Derivatives of the Jastrow factors are
\[
\frac{\nabla_k \psi_C}{\psi_C} = \sum_{  i=1, \ i \neq k}^n \frac{\gamma_{ik}}{(1 + \beta r_{ik})^2 r_{ik}} \Big( \vec e_1 (x_k -x_i) + \vec e_2 (y_k -y_i) \Big)
\]
\[
\frac{\nabla^2_k \psi_C}{\psi_C} = \left[ \sum_{j=1, j \neq k}^n \frac{\gamma_{kj}(x_k-x_j) }{(1 + \beta r_{kj})^2 r_{kj}} \right]^2 + \left[ \sum_{j=1, j \neq k}^n \frac{\gamma_{kj}(y_k-y_j) }{(1 + \beta r_{kj})^2 r_{kj}} \right]^2 + \sum_{j=1, j \neq k}^n \frac{\gamma_{kj}(1 - \beta r_{kj}) }{r_{kj} (1 + \beta r_{kj})^3}
\]
Auxiliary expressions for the steepest decent method are
\[
\frac{1}{\psi_T}\frac{\partial\psi_T}{\partial \alpha} = \frac{1}{\det D_\uparrow}\frac{\partial \det D_\uparrow}{\partial \alpha} + \frac{1}{\det D_\downarrow}\frac{\partial \det D_\downarrow}{\partial \alpha}, \qquad \frac{1}{\psi_T}\frac{\partial\psi_T}{\partial \beta} = - \sum_{i=1}^n\sum_{j=i+1}^n \gamma_{ij} \left( \frac{r_{ij}}{1 + \beta r_{ij}} \right)^2
\]
\begin{equation}
\frac{1}{\det D_\uparrow}\frac{\partial \det D_\uparrow}{\partial \alpha} = \sum_{i=0}^{n/2-1}\sum_{j=0}^{n/2-1} d^{-1}_{ij} \frac{\partial \phi_{2i}}{\partial \alpha} (\vec{r}_{2j}), \quad \frac{1}{\det D_\downarrow}\frac{\partial \det D_\downarrow}{\partial \alpha} = \sum_{i=0}^{n/2-1}\sum_{j=0}^{n/2-1} d^{-1}_{ij} \frac{\partial \phi_{2i+1}}{\partial \alpha}(\vec{r}_{2j+1} ) \label{eq:steepest1}
\end{equation}
\begin{equation}
\frac{\partial}{\partial \alpha} \phi_{i} (\vec{r}_j) = \left( x_j n_{xi} \phi_{n_{xi}-1,n_{yi}} (\vec{r}_j) + y_j n_{yi} \phi_{n_{xi},n_{yi}-1} (\vec{r}_j) \right)\left( \frac{\omega}{\alpha} \right)^{1/2} - \frac{1}{2}\omega r_j^2 \phi_{n_{xi},n_{yi}} (\vec{r}_j) \label{eq:steepest2}
\end{equation}
\subsubsection*{\uppercase{Proofs}}

\begin{theorem}[Hastings-Metropolis theorem]
Suppose that $C\pi_i$ is a discrete probability distribution. If $q_{ij}$ is any irreducible transition probability matrix, and $X_n$ is a Markov chain with transition probability matrix 
\begin{equation}
P_{ij} = \ccases {
\alpha_{  ij} q_{  ij} , \quad &j\neq i\\
q_{  ii} + \sum_{k=0}^\infty q_{  ik}(1 - \alpha_{  ik})\quad & j=i
},\qquad \text{where} \qquad \alpha_{  ij} = \min \left( \frac{\pi_j q_{  ji}}{\pi_i q_{  ij}} , 1\right), \label{eq:metropolis}
\end{equation}
then $X_n$ is time reversible with stationary distribution $\pi_i$. \label{thm:metropolis}
\end{theorem}
\begin{proof}
Assume that the hypothesis is true. Then in particular $q_{  ij} \neq 0$ for all $i,j$ since $q$ is irreducible. Notice that if 
\[
\frac{\pi_j q_{  ji}}{\pi_i q_{  ij}} = 1,
\]
then there is nothing to prove since then $\alpha_{ij} = 1$ and $\alpha_{ji} = 1$, and therefore
\begin{equation}
\pi_iP_{  ij} = \pi_jP_{  ji} \label{eq:detailed_balance}
\end{equation} 
is automatic. Hence it suffices to prove \eqref{eq:detailed_balance} for the two cases
\[
\frac{\pi_j q_{  ji}}{\pi_i q_{  ij}} > 1 \qquad \text{and} \qquad \frac{\pi_j q_{  ji}}{\pi_i q_{  ij}} < 1,
\]
separately. Suppose first that $\pi_j q_{  ji} > \pi_i q_{  ij}$ $(\dagger)$. Write:
\[
\pi_iP_{  ij} \stackrel{\eqref{eq:metropolis}}{=} \pi_i  q_{  ij} \alpha_{  ij} \stackrel{\eqref{eq:metropolis}(\dagger)}{=} \pi_i q_{  ij} \cdot 1 =
\pi_i q_{  ij}  \frac{\alpha_{  ji}}{\alpha_{  ji}} = \alpha_{  ji}\pi_i q_{  ij}  \frac{1}{\alpha_{  ji}} \stackrel{(\dagger)}{=} \alpha_{  ji}\pi_i q_{  ij}  \frac{\pi_j q_{  ji}}{\pi_i q_{  ij}} = \alpha_{  ji}  \pi_j q_{  ji} \stackrel{\eqref{eq:metropolis}}{=} \pi_jP_{  ji}. 
\]
In the case that $\pi_j q_{  ji} < \pi_i q_{  ij}$ $(\ddagger)$, write
\[
\pi_iP_{  ij} \stackrel{\eqref{eq:metropolis}}{=} \pi_i  q_{  ij} \alpha_{  ij} \stackrel{\eqref{eq:metropolis}(\ddagger)}{=} \pi_i q_{  ij} \frac{\pi_j q_{  ji}}{\pi_i q_{  ij}} = \pi_j q_{  ji} = \pi_j q_{  ji} \cdot 1 \stackrel{\eqref{eq:metropolis}(\ddagger)}{=} \pi_j q_{  ji} \cdot \alpha_{  ji} = \pi_jP_{  ji},
\]
which means $X_n$ is time reversible with stationary probability $\pi_i$.
\end{proof}
\begin{lemma*}
Suppose $X$ and $Y$ are random variables with finite variance, then
\[
| \cov (X,Y) |^2 \leq \cov (X,X) \cov (Y,Y)
\]
\label{lemma:inequality}
\end{lemma*}
\begin{proof}
Suppose $\Omega$ denotes the vector space of all real random variables with finite variance over the real numbers. Suppose $X,Y \in \Omega$ and define
\begin{equation}
(X,Y) = \E{X Y}. \label{eq:innerproduct}
\end{equation}
To see that $\big( \Omega, (\cdot,\cdot) \big)$ is an inner product space, note that
\begin{align*}
\text{Positivity:} \qquad &(X,X) = \E{X^2} \geq 0 \qquad =0 \quad \text{if and only if} \quad X=0 \quad \text{almost surely.}\\ 
\text{Symmetry:} \qquad &(X,Y) = \E{XY} = \E{YX} = (Y,X)\\ 
\text{Bilinearity:} \qquad &(aX + bY,Z) = \E{(aX + bY)Z} = a\E{XZ} + b\E{YZ} = a(X,Z) + b(YZ) &
\end{align*}
This proves that $( \cdot, \cdot )$ is an inner product on $\Omega$ \parencite{mcdonald_course_2012}. In particular, that means we can use Cauchy-Schwarz inequality \parencite{lay_linear_2012}. Now use the definition of covariance $(*)$ and apply Cauchy-Schwarz inequality $(**)$. Write
\begin{align*}
| \cov (X,Y) |^2 &\stackrel{(*)}{=} | \E{(X - \E{X})(Y - \E{Y})} |^2\qquad 0  \stackrel{\eqref{eq:innerproduct}}{=} | (X-\E{X}, Y-\E{Y}) |^2 \\
&\stackrel{(**)}{\leq}  (X-\E{X}, X-\E{X})(X-\E{Y}, Y-\E{Y}) \stackrel{(*)\eqref{eq:innerproduct}}{=} \cov (X,X) \cov (Y,Y)
\end{align*}
Which is what we wanted.
\end{proof}
\printbibliography
\end{document}